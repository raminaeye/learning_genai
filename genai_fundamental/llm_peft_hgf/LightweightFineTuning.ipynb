{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRA\n",
    "* Model: distillbert-base-uncased\n",
    "* Evaluation approach: classification\n",
    "* Fine-tuning dataset:  yelp reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee8c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, TaskType,PeftModel\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3a3afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset yelp review \n",
    "ds = load_dataset(\"fancyzhx/yelp_polarity\", split=\"train[42%:50%]\").train_test_split(test_size=0.2, shuffle=True, seed=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3fb71856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6100f5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b891cebc0b4fa0b4c9dfee8902efda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# apply tokenizer to dataset\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = ds[split].map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8bfc6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2ae92",
   "metadata": {},
   "source": [
    "# Training and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999e7ff4e5694c138cb0b5a97b7dcfdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model from huggingface\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                           num_labels=2,\n",
    "                                                           id2label={0: \"bad\", 1: \"good\"},\n",
    "                                                           label2id={\"bad\": 0, \"good\": 1}).to(\"cuda\")\n",
    "\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db77ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/classifier_head\",\n",
    "        # Set the learning rate\n",
    "        learning_rate = 0.001,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size = 16,\n",
    "        per_device_eval_batch_size = 8,\n",
    "        # Evaluate and save the model after each epoch\n",
    "        evaluation_strategy = 'epoch',\n",
    "        save_strategy = 'epoch',\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.001,\n",
    "        load_best_model_at_end=True,\n",
    "        fp16=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df1b99",
   "metadata": {},
   "source": [
    "# Training the classifier head\n",
    "\n",
    "Instruction mentions to just load the model and evaluate it, however, the pretrained model needs a classifier on top which is not trained. So, to make the evaluation meaningful, I train the classifier head first before evaluating the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21d8ac30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2240' max='2240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2240/2240 05:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.240385</td>\n",
       "      <td>0.900335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2240, training_loss=0.290946900844574, metrics={'train_runtime': 321.6664, 'train_samples_per_second': 111.42, 'train_steps_per_second': 6.964, 'total_flos': 4747631567831040.0, 'train_loss': 0.290946900844574, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c1cae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_name = 'distillbert_' + 'classifier'\n",
    "model.save_pretrained('./data/' + model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea7fca02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1120' max='1120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1120/1120 00:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.24038466811180115,\n",
       " 'eval_accuracy': 0.9003348214285715,\n",
       " 'eval_runtime': 58.5362,\n",
       " 'eval_samples_per_second': 153.068,\n",
       " 'eval_steps_per_second': 19.133,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a4bfee",
   "metadata": {},
   "source": [
    "### Load model from drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "305ca890",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distillbert_classifier'\n",
    "model_path = '/workspace/data/' + model_name\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,\n",
    "                                                           num_labels=2,\n",
    "                                                           id2label={0: \"bad\", 1: \"good\"},\n",
    "                                                           label2id={\"bad\": 0, \"good\": 1}).to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42312225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model \n",
    "df = pd.DataFrame(tokenized_dataset[\"test\"])\n",
    "df = df[[\"text\", \"label\"]]\n",
    "tokenized_text = tokenizer(list(df[\"text\"]), padding=\"max_length\", truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "98a8105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate saved model\n",
    "batch_size = 32\n",
    "b = [0, batch_size]\n",
    "\n",
    "len_samples = len(tokenized_text['input_ids'])\n",
    "\n",
    "predicted_labels = []\n",
    "model.eval()\n",
    "while b[1] < len_samples:\n",
    "    print(round(b[1]/len_samples, 3), end = '\\r')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokenized_text['input_ids'][b[0]:b[1]].to('cuda'))\n",
    "        predicted_labels.append(outputs.logits.argmax(-1))\n",
    "        b[0] += batch_size\n",
    "        b[1] += batch_size\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokenized_text['input_ids'][b[0]:b[1]].to('cuda'))\n",
    "    predicted_labels.append(outputs.logits.argmax(-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9000f2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7338169642857143\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model \n",
    "predictions = np.concatenate(([list(np.array(pp.cpu().numpy())) for pp in predicted_labels]))\n",
    "accuracy = np.sum(predictions == list(df['label']))/len(df['label'])\n",
    "print(f\"accuracy {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f69c82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Best coffee in town. You never would think a donut shop would be known for their coffee but Dunkin Donuts has excellent coffee. The donuts are slightly above average. But over all a good quick on the go breakfast.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check some of the texts that were incorrectly labeled \n",
    "df['predicted_label'] = predictions\n",
    "incorrect_predict_reviews = df[df['label'] != predictions]\n",
    "incorrect_predict_reviews.iloc[0]['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2af6a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from peft import get_peft_model\n",
    "\n",
    "model_name = 'distillbert_classifier'\n",
    "model_path = '/workspace/data/' + model_name\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,\n",
    "                                                           num_labels=2,\n",
    "                                                           id2label={0: \"bad\", 1: \"good\"},\n",
    "                                                           label2id={\"bad\": 0, \"good\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62609c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,405,444 || all params: 67,768,324 || trainable%: 2.073895172617815\n"
     ]
    }
   ],
   "source": [
    "# Create a PEFT Model\n",
    "lora_config = LoraConfig(target_modules=[\"q_lin\", \"k_lin\",\"v_lin\"], # Which layer to apply LoRA, usually only apply on MultiHead Attention Layer\n",
    "                         task_type=TaskType.SEQ_CLS )\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config).to('cuda')\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be3479fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10704"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(model)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8960' max='8960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8960/8960 13:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.171411</td>\n",
       "      <td>0.950335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/lora/checkpoint-8960 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8960, training_loss=0.22954959869384767, metrics={'train_runtime': 816.1692, 'train_samples_per_second': 43.912, 'train_steps_per_second': 10.978, 'total_flos': 4837177829621760.0, 'train_loss': 0.22954959869384767, 'epoch': 1.0})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the PEFT Model\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/lora\",\n",
    "        learning_rate = 0.001,\n",
    "        per_device_train_batch_size = 4,\n",
    "        per_device_eval_batch_size = 4,\n",
    "        evaluation_strategy = 'epoch',\n",
    "        save_strategy = 'epoch',\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.001,\n",
    "        load_best_model_at_end=True,\n",
    "        fp16=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2240' max='2240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2240/2240 01:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.17141121625900269,\n",
       " 'eval_accuracy': 0.9503348214285714,\n",
       " 'eval_runtime': 73.5817,\n",
       " 'eval_samples_per_second': 121.769,\n",
       " 'eval_steps_per_second': 30.442,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the PEFT model\n",
    "model_name = 'distillbert_' + 'lora'\n",
    "lora_model.save_pretrained('./data/' + model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PEFT Model\n",
    "model_name = './data/' +  'distillbert_' + \"lora\"\n",
    "\n",
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(model_name).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d818daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model \n",
    "df = pd.DataFrame(tokenized_dataset[\"test\"])\n",
    "df = df[[\"text\", \"label\"]]\n",
    "tokenized_text = tokenizer(list(df[\"text\"]), padding=\"max_length\", truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996\r"
     ]
    }
   ],
   "source": [
    "# Evaluate saved model\n",
    "batch_size = 32\n",
    "b = [0, batch_size]\n",
    "\n",
    "len_samples = len(tokenized_text['input_ids'])\n",
    "\n",
    "predicted_labels = []\n",
    "lora_model.eval()\n",
    "while b[1] < len_samples:\n",
    "    print(round(b[1]/len_samples, 3), end = '\\r')\n",
    "    with torch.no_grad():\n",
    "        outputs = lora_model(tokenized_text['input_ids'][b[0]:b[1]].to('cuda'))\n",
    "        predicted_labels.append(outputs.logits.argmax(-1))\n",
    "        b[0] += batch_size\n",
    "        b[1] += batch_size\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = lora_model(tokenized_text['input_ids'][b[0]:b[1]].to('cuda'))\n",
    "    predicted_labels.append(outputs.logits.argmax(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8800223214285714\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model \n",
    "predictions = np.concatenate(([list(np.array(pp.cpu().numpy())) for pp in predicted_labels]))\n",
    "accuracy = np.sum(predictions == list(df['label']))/len(df['label'])\n",
    "print(f\"accuracy {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "06739072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Islands \\\\\"\"fine\\\\\"\" burgers.... Should drop the \\\\\"\"fine\\\\\"\" in their name. This is simply another version of Chilis or AppleBee\\'s. Frozen burgers, frozen fries, frozen veg. - overall pretty blah :-('"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check some of the texts that were incorrectly labeled \n",
    "df['lora_predicted_label'] = predictions\n",
    "incorrect_predict_reviews = df[df['label'] != predictions]\n",
    "incorrect_predict_reviews.iloc[0]['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b77a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
